{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalakhovDenis/DLS-DL2/blob/main/11.%20%D0%94%D0%97.%20%D0%9F%D1%80%D0%B5%D0%B4%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B8%20%D0%B4%D0%BE%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D1%8B%D1%85%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9/11.1%20%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80%D1%8B/Malakhov%5Bhomework4%5DAttention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"400\"></p>\n",
        "\n",
        "# Глубокое обучение. Часть 2\n",
        "# Домашнее задание по теме \"Механизм внимания\""
      ],
      "metadata": {
        "id": "Ji8KtYOVGs8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
        "\n",
        "В этом задании вы будете решать задачу классификации математических задач по темам (многоклассовая классификация) с помощью Transformer.\n",
        "\n",
        "В качестве датасета возьмем датасет математических задач по разным темам. Нам необходим следующий файл:\n",
        "\n",
        "[Файл с классами](https://docs.google.com/spreadsheets/d/13YIbphbWc62sfa-bCh8MLQWKizaXbQK9/edit?usp=drive_link&ouid=104379615679964018037&rtpof=true&sd=true)"
      ],
      "metadata": {
        "id": "UAr-M8_1GJ6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint:** не перезаписывайте модели, которые вы получите на каждом из этапов этого дз. Они ещё понадобятся."
      ],
      "metadata": {
        "id": "1fybMcmV0YRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Импорт библиотек и предобработка данных"
      ],
      "metadata": {
        "id": "Kn8s6Wl0p3bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard evaluate transformers datasets pymorphy2 -q"
      ],
      "metadata": {
        "id": "k1u-CEGlqC4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AutKSA38qGpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки для дальнейшего анализа\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import nltk\n",
        "import pymorphy2\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import evaluate\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, get_scheduler\n",
        "\n",
        "sns.set(palette='summer')\n",
        "\n",
        "print('Все библиотеки импортированы')"
      ],
      "metadata": {
        "id": "WU8FPGnQqI0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "4EHWmIF2qMD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "def seed_torch(seed):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(SEED)"
      ],
      "metadata": {
        "id": "1j7G36mRqOGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузим данные, посмотрим на их структуру\n",
        "\n",
        "data = pd.read_excel('./data_problems_translated.xlsx', index_col=0)\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Gh2IuR4VqTpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns = ['text','label']"
      ],
      "metadata": {
        "id": "qoYi9Yx8qVfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Количество уникальных лейблов: {data.label.nunique()}')\n",
        "data.label.value_counts()"
      ],
      "metadata": {
        "id": "O94iLLm6qXFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].fillna('').astype(str)\n",
        "data['len_text'] = data.text.apply(lambda x: len(str(x)))\n",
        "display(data.describe())\n",
        "data.len_text.hist()\n",
        "None"
      ],
      "metadata": {
        "id": "jQugd9sOqY3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка на дубликаты в данных\n",
        "\n",
        "data.shape, data.drop_duplicates().shape, data.drop_duplicates('text').shape"
      ],
      "metadata": {
        "id": "9rhFQgAPqbCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('text').label.nunique().sort_values(ascending=False).reset_index().head()"
      ],
      "metadata": {
        "id": "NZo1Zj55qcxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes, data.isna().sum()"
      ],
      "metadata": {
        "id": "UelQWIN1qeRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data.text.str.contains(\"It's okay. It's okay\")].drop_duplicates(['text','label'])"
      ],
      "metadata": {
        "id": "dqwGUR0zqfnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:\n",
        "* В данных есть дубликаты по задачам, однако каждый из них имеет несколько Тем! Таким образом решение задачи может быть либо задача Multilabel, либо при вычистке дубликатов можно решать задачу Multiclass.\n",
        "* Длина предложения самого текста задачи от 0 до 2506 симовлов, при 50 и 75 квантиле 179 и 286 соответственно. Таким образом падить будет не очень хорошей идеей, надо фильтрануть наблюдения по длине предложения"
      ],
      "metadata": {
        "id": "9k2zBCwzqhs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "1) Удалим Дубликаты по конверсии лейблов"
      ],
      "metadata": {
        "id": "rw-vuzx5qmsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Заведем ранжировку для каждого из лейблов и возьмем тот, где первого, где меньше количество наблюдений\n",
        "\n",
        "rang_dict = {k:v for v,k in enumerate(data.label.value_counts().sort_values().index)}\n",
        "data['rang_label'] = data.label.map(rang_dict)"
      ],
      "metadata": {
        "id": "8U0ustE6qo46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data.sort_values(['text','rang_label']).drop_duplicates('text',keep='first').drop(columns='rang_label').label.value_counts()\n",
        ",data.sort_values(['text','rang_label']).drop_duplicates('text',keep='last').drop(columns='rang_label').label.value_counts())"
      ],
      "metadata": {
        "id": "P99B36nQqq6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Заведем ранжировку для каждого из лейблов и возьмем тот, где первого, где меньше количество наблюдений\n",
        "\n",
        "rang_dict = {k:v for v,k in enumerate(data.label.value_counts().sort_values().index)}\n",
        "data['rang_label'] = data.label.map(rang_dict)\n",
        "data = data.sort_values(['text','rang_label']).drop_duplicates('text',keep='first').drop(columns='rang_label')"
      ],
      "metadata": {
        "id": "IAR27mX5qtZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label_to_ind = {label:ind for ind,label in enumerate(data.label.unique())}\n",
        "int_to_label = {ind: label for label, ind in label_to_ind.items()}\n",
        "NUM_CLASSES = len(label_to_ind)\n",
        "\n",
        "data['label'] = data['label'].map(label_to_ind)\n",
        "\n",
        "train_data, eval_data = train_test_split(data[['text','label']], stratify=data['label'], test_size=0.15, random_state=SEED, shuffle=True)"
      ],
      "metadata": {
        "id": "_pVkMGscqu_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "2) Посмотрим на выбросы по длинам предложений"
      ],
      "metadata": {
        "id": "DlpWzseXqwyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of code is from Seminar Attention and Transformers\n",
        "\n",
        "ENG_STOP_WORDS = set(stopwords.words('english'))\n",
        "PUNCT_WORD_TOKENIZER = nltk.WordPunctTokenizer() # for preprocess\n",
        "# for broadening your horizons use it for lemmatization\n",
        "MORPH_ANALYZER = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    nums_filtered_text = re.sub(r'[0-9]+', '', text.lower())\n",
        "    punct_filtered_text = ''.join(\n",
        "        [ch for ch in nums_filtered_text if ch not in string.punctuation]\n",
        "    )\n",
        "    tokens = PUNCT_WORD_TOKENIZER.tokenize(punct_filtered_text)\n",
        "    filtr_stop_words_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in tokens\n",
        "                             if token not in ENG_STOP_WORDS]\n",
        "    norm_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in filtr_stop_words_tokens]\n",
        "\n",
        "    return f\"[CLS] {' '.join(norm_tokens)}\""
      ],
      "metadata": {
        "id": "ZrAx4Y9Yqxnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Слелаем расчет на Интерквартильный размах и посчитаем выбросы по длине предложений\n",
        "\n",
        "from scipy.stats import iqr\n",
        "\n",
        "data = data[data.len_text>=data.len_text.quantile(0.01)]\n",
        "\n",
        "print(data.len_text.quantile(0.25)-iqr(data.len_text),\n",
        "      data.len_text.quantile(0.25),\n",
        "      data.len_text.quantile(0.5),\n",
        "      data.len_text.quantile(0.75),\n",
        "      data.len_text.quantile(0.75)+iqr(data.len_text))"
      ],
      "metadata": {
        "id": "rbcuJbBNq1ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of code is from Seminar Attention and Transformers\n",
        "## Changed hug_dataset to my_dataset\n",
        "\n",
        "MAX_LENGTH = int(data.len_text.quantile(0.75))\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               my_dataset,\n",
        "               tokenizer,\n",
        "               device=DEVICE):\n",
        "\n",
        "    self.text = my_dataset['text'].apply(lambda x: preprocess_text(x)).tolist()\n",
        "    self.target = my_dataset['label'].tolist()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.device = device\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    prep_text = self.text[idx]\n",
        "    target = self.target[idx]\n",
        "\n",
        "    tokenized_text = self.tokenizer(text=prep_text,\n",
        "                                    padding=\"max_length\",\n",
        "                                    max_length=MAX_LENGTH,\n",
        "                                    truncation=True,\n",
        "                                    return_tensors='pt')\n",
        "\n",
        "    text_ids = tokenized_text['input_ids'].flatten()\n",
        "    attention_mask = tokenized_text['attention_mask'].flatten()\n",
        "\n",
        "    return text_ids, attention_mask, torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)"
      ],
      "metadata": {
        "id": "FAOb47R2q3HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1 (2 балла)\n",
        "\n",
        "Напишите кастомный класс для модели трансформера для задачи классификации, использующей в качествке backbone какую-то из моделей huggingface.\n",
        "\n",
        "Т.е. конструктор класса должен принимать на вход название модели и подгружать её из huggingface, а затем использовать в качестве backbone (достаточно возможности использовать в качестве backbone те модели, которые упомянуты в последующих пунктах)"
      ],
      "metadata": {
        "id": "t395freCxpOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### This is just an interface example. You may change it if you want.\n",
        "\n",
        "class TransformerClassificationModel(nn.Module):\n",
        "    def __init__(base_transformer_model: Union[str, nn.Module]):\n",
        "        self.backbone = #...\n",
        "        # YOUR CODE: create additional layers for classfication\n",
        "\n",
        "    def forward(inputs, ...):\n",
        "        # YOUR CODE: propagate inputs through the model. Return dict with logits\n",
        "\n",
        "        outputs = {<YOUR CODE>}\n",
        "        return # YOUR CODE"
      ],
      "metadata": {
        "id": "eX4VGWquyiMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2 (1 балл)\n",
        "\n",
        "Напишите функцию заморозки backbone у модели (если необходимо, возвращайте из функции модель)"
      ],
      "metadata": {
        "id": "Vd3kxX6hy0d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_backbone_function(model: TransformerClassificationModel):\n",
        "    pass"
      ],
      "metadata": {
        "id": "U8IuDosbzKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3 (2 балла)\n",
        "\n",
        "Напишите функцию, которая будет использована для тренировки (дообучения) трансформера (TransformerClassificationModel). Функция должна поддерживать обучение с замороженным и размороженным backbone."
      ],
      "metadata": {
        "id": "kybkw6MSzd-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def train_transformer(transformer_model, freeze_backbone=True)\n",
        "    model = copy.copy(transformer_model)\n",
        "    ### YOUR CODE IS HERE\n",
        "\n",
        "    return finetuned_model"
      ],
      "metadata": {
        "id": "EDhrD0BHzxi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4 (1 балл)\n",
        "\n",
        "Проверьте вашу функцию из предыдущего пункта, дообучив двумя способами\n",
        "*cointegrated/rubert-tiny2* из huggingface."
      ],
      "metadata": {
        "id": "eUqhI4mV_RTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rubert_tiny_transformer_model = #...\n",
        "rubert_tiny_finetuned_with_freezed_backbone = train_transformer(rubert_tiny_transformer_model, freeze_backbone=True)\n",
        "\n",
        "rubert_tiny_transformer_model = #...\n",
        "rubert_tiny_full_finetuned = train_transformer(rubert_tiny_transformer_model, freeze_backbone=False)"
      ],
      "metadata": {
        "id": "nuxOCBQHAKZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 5 (1 балл)\n",
        "\n",
        "Обучите *tbs17/MathBert* (с замороженным backbone и без заморозки), проанализируйте результаты. Сравните скоры с первым заданием. Получилось лучше или нет? Почему?"
      ],
      "metadata": {
        "id": "zRi7tkoOAjon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE (probably, similar on the previous step)"
      ],
      "metadata": {
        "id": "XKtd3YgNA14E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 6 (1 балл)\n",
        "\n",
        "Напишите функцию для отрисовки карт внимания первого слоя для моделей из задания"
      ],
      "metadata": {
        "id": "EuU6Di26017B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_first_layer_attention_maps(attention_head_ids: List, text: str, model: TransformerClassificationModel):\n",
        "    pass"
      ],
      "metadata": {
        "id": "guzGxfcV1Cba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 7 (1 балл)\n",
        "\n",
        "Проведите инференс для всех моделей **ДО ДООБУЧЕНИЯ** на 2-3 текстах из датасета. Посмотрите на головы Attention первого слоя в каждой модели на выбранных текстах (отрисуйте их отдельно).\n",
        "\n",
        "Попробуйте их проинтерпретировать. Какие связи улавливают карты внимания? (если в модели много голов Attention, то проинтерпретируйте наиболее интересные)"
      ],
      "metadata": {
        "id": "Iu0adKw4BLtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE"
      ],
      "metadata": {
        "id": "U2gEF3vkB6eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 8 (1 балл)\n",
        "\n",
        "Сделайте то же самое для дообученных моделей. Изменились ли карты внимания и связи, которые они улавливают? Почему?"
      ],
      "metadata": {
        "id": "pBNVrOpCCLqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE"
      ],
      "metadata": {
        "id": "F5229WBICWEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}